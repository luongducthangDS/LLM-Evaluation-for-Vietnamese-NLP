import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from peft import PeftModel

# ======================
# CONFIG
# ======================
BASE_MODEL_NAME = "NlpHUST/gpt2-vietnamese"
ADAPTER_PATH = "lora_gpt2_viquad" # Th∆∞ m·ª•c b·∫°n ƒë√£ l∆∞u model sau khi train
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {DEVICE}")

# ======================
# 1. LOAD TOKENIZER
# ======================
# L∆∞u √Ω: Load tokenizer t·ª´ th∆∞ m·ª•c ƒë√£ train xong ƒë·ªÉ gi·ªØ ƒë√∫ng config (pad_token, vocab_size)
print("Loading tokenizer...")
try:
    tokenizer = GPT2Tokenizer.from_pretrained(ADAPTER_PATH)
except:
    # Fallback n·∫øu kh√¥ng t√¨m th·∫•y tokenizer trong th∆∞ m·ª•c output
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y tokenizer ƒë√£ l∆∞u, load t·ª´ base model...")
    tokenizer = GPT2Tokenizer.from_pretrained(BASE_MODEL_NAME)
    tokenizer.pad_token = tokenizer.eos_token

# ======================
# 2. LOAD BASE MODEL & RESIZE
# ======================
print("Loading base model...")
base_model = GPT2LMHeadModel.from_pretrained(
    BASE_MODEL_NAME,
    device_map="auto" if torch.cuda.is_available() else None,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
)

# QUAN TR·ªåNG: Ph·∫£i resize l·∫°i embeddings c·ªßa base model cho kh·ªõp v·ªõi tokenizer
# N·∫øu b·ªè qua b∆∞·ªõc n√†y, khi load adapter s·∫Ω b·ªã l·ªói l·ªách k√≠ch th∆∞·ªõc tensor
base_model.resize_token_embeddings(len(tokenizer))

# ======================
# 3. LOAD LORA ADAPTER
# ======================
print("Loading LoRA adapter...")
model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)

# Chuy·ªÉn model sang ch·∫ø ƒë·ªô ƒë√°nh gi√° (kh√¥ng train n·ªØa)
model.eval()
if DEVICE == "cuda":
    model.to(DEVICE)

print("‚úÖ Model ƒë√£ s·∫µn s√†ng!")

# ======================
# H√ÄM SINH VƒÇN B·∫¢N
# ======================
def generate_answer(context, question):
    # Format prompt y h·ªát nh∆∞ l√∫c train
    prompt = (
        f"Ng·ªØ c·∫£nh:\n{context}\n\n"
        f"C√¢u h·ªèi: {question}\n"
        f"Tr·∫£ l·ªùi:"
    )

    # Tokenize input
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].to(DEVICE)
    attention_mask = inputs["attention_mask"].to(DEVICE)

    # Generate
    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=100,      # ƒê·ªô d√†i t·ªëi ƒëa c·ªßa c√¢u tr·∫£ l·ªùi
            do_sample=True,          # Random sampling ƒë·ªÉ c√¢u vƒÉn t·ª± nhi√™n h∆°n
            top_p=0.9,               # L·∫•y c√°c t·ª´ c√≥ x√°c su·∫•t c·ªông d·ªìn 90%
            temperature=0.7,         # ƒê·ªô s√°ng t·∫°o (th·∫•p = ch√≠nh x√°c, cao = s√°ng t·∫°o)
            repetition_penalty=1.2,  # Tr√°nh l·∫∑p t·ª´
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    # Decode k·∫øt qu·∫£
    # Ch·ªâ l·∫•y ph·∫ßn m·ªõi sinh ra (b·ªè ph·∫ßn prompt ban ƒë·∫ßu)
    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # C·∫Øt b·ªè ph·∫ßn prompt ƒë·ªÉ ch·ªâ l·∫•y c√¢u tr·∫£ l·ªùi
    answer = generated_text.replace(prompt, "").strip()

    return answer

# ======================
# TEST TH·ª¨
# ======================
context_text = """
Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin (UIT) l√† m·ªôt tr∆∞·ªùng ƒë·∫°i h·ªçc c√¥ng l·∫≠p t·∫°i Vi·ªát Nam, tr·ª±c thu·ªôc ƒê·∫°i h·ªçc Qu·ªëc gia Th√†nh ph·ªë H·ªì Ch√≠ Minh.
Tr∆∞·ªùng ƒë∆∞·ª£c th√†nh l·∫≠p v√†o ng√†y 8 th√°ng 6 nƒÉm 2006. UIT chuy√™n ƒë√†o t·∫°o v·ªÅ c√¥ng ngh·ªá th√¥ng tin v√† truy·ªÅn th√¥ng.
"""

question_text = "Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin ƒë∆∞·ª£c th√†nh l·∫≠p khi n√†o?"

print("\n" + "="*30)
print("CONTEXT:", context_text)
print("QUESTION:", question_text)
print("="*30)

answer = generate_answer(context_text, question_text)

print(f"\nü§ñ MODEL TR·∫¢ L·ªúI:\n{answer}")