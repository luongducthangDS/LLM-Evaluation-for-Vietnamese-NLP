# ============================================================
# Inference VietAI/vit5-base + LoRA (Question Answering)
# ============================================================

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel

# ======================
# CONFIG
# ======================
BASE_MODEL_NAME = "VietAI/vit5-base"
ADAPTER_PATH = "lora_vit5_viquad"   # th∆∞ m·ª•c ƒë√£ save sau khi finetune
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {DEVICE}")

# ======================
# 1. LOAD TOKENIZER
# ======================
print("Loading tokenizer...")
try:
    # ∆Øu ti√™n load tokenizer t·ª´ output_dir ƒë·ªÉ gi·ªØ config nh·∫•t qu√°n
    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)
except:
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y tokenizer trong th∆∞ m·ª•c adapter, load t·ª´ base model...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)

# ======================
# 2. LOAD BASE MODEL
# ======================
print("Loading base model...")
base_model = AutoModelForSeq2SeqLM.from_pretrained(
    BASE_MODEL_NAME,
    device_map="auto" if DEVICE == "cuda" else None,
    torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
)

# ======================
# 3. LOAD LORA ADAPTER
# ======================
print("Loading LoRA adapter...")
model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)

model.eval()
if DEVICE == "cuda":
    model.to(DEVICE)

print("‚úÖ Model ƒë√£ s·∫µn s√†ng inference!")

# ======================
# H√ÄM INFERENCE QA
# ======================
def generate_answer(context, question,
                    max_new_tokens=128,
                    num_beams=4):
    """
    Sinh c√¢u tr·∫£ l·ªùi cho b√†i to√°n QA theo ƒë√∫ng format l√∫c train
    """

    # Format input gi·ªëng preprocess khi train
    input_text = (
        "tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh sau:\n\n"
        f"Ng·ªØ c·∫£nh: {context}\n\n"
        f"C√¢u h·ªèi: {question}"
    )

    # Tokenize
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
    ).to(DEVICE)

    # Generate (T5-style)
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            num_beams=num_beams,      # beam search ‚Üí ·ªïn ƒë·ªãnh, ƒë√∫ng ng·ªØ nghƒ©a
            early_stopping=True,
            no_repeat_ngram_size=3,   # tr√°nh l·∫∑p
        )

    # Decode output
    answer = tokenizer.decode(
        output_ids[0],
        skip_special_tokens=True
    ).strip()

    return answer

# ======================
# TEST TH·ª¨
# ======================
context_text = """
Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin (UIT) l√† m·ªôt tr∆∞·ªùng ƒë·∫°i h·ªçc c√¥ng l·∫≠p t·∫°i Vi·ªát Nam,
tr·ª±c thu·ªôc ƒê·∫°i h·ªçc Qu·ªëc gia Th√†nh ph·ªë H·ªì Ch√≠ Minh.
Tr∆∞·ªùng ƒë∆∞·ª£c th√†nh l·∫≠p v√†o ng√†y 8 th√°ng 6 nƒÉm 2006.
UIT chuy√™n ƒë√†o t·∫°o v·ªÅ c√¥ng ngh·ªá th√¥ng tin v√† truy·ªÅn th√¥ng.
"""

question_text = "Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin ƒë∆∞·ª£c th√†nh l·∫≠p khi n√†o?"

print("\n" + "=" * 40)
print("CONTEXT:\n", context_text)
print("QUESTION:", question_text)
print("=" * 40)

answer = generate_answer(context_text, question_text)

print("\nü§ñ MODEL TR·∫¢ L·ªúI:")
print(answer)
